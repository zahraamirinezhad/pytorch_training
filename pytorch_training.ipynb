{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "16QKE5QIfO61"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quickstart**"
      ],
      "metadata": {
        "id": "eFMJY2i0CsHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# root=\"data\"\t=> The directory where the dataset will be stored. If \"data\" doesn’t exist, it will be created.\n",
        "# train=True =>\tThis tells PyTorch to load the training set (60,000 images).\n",
        "# download=True =>\tIf the dataset is not already present, download it from the internet.\n",
        "# transform=ToTensor() =>\tApply a transformation that converts each image (PIL format) into a PyTorch tensor and normalizes pixel values to [0.0, 1.0]."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgPV4pdpj72h",
        "outputId": "a580919d-2e9e-4ad1-f54b-3531f160c5be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.7MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 210kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.91MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 24.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "# This sets the number of samples per batch.\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "# This wraps the training_data (which is a FashionMNIST dataset) in a DataLoader.\n",
        "# What DataLoader does:\n",
        "  # Batches the data into size batch_size\n",
        "  # Allows iterating over the dataset using a for loop\n",
        "  # Optionally shuffles or loads data in parallel (more on this later)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "# output [64, 1, 28, 28]: Each batch contains 64 grayscale images (1 channel, 28x28 pixels) and their 64 labels."
      ],
      "metadata": {
        "id": "HcuQto-uoeiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e0d1e6-c3e1-4214-9fa8-e91e4a5f5ecf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "# Checks if any hardware accelerator (GPU or Apple Silicon, etc.) is available.\n",
        "# If yes, it uses it (e.g., \"cuda\" for NVIDIA GPU, \"mps\" for Apple GPU).\n",
        "# If not, it defaults to \"cpu\".\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "  # define a class that inherits from nn.Module, which is the base class for all PyTorch models.\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        # nn.Flatten() converts the 2D image (28×28) into a 1D vector (784 values).\n",
        "        # Shape change: [batch_size, 1, 28, 28] ⟶ [batch_size, 784]\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512), # input layer\n",
        "            nn.ReLU(),             # activation function\n",
        "            nn.Linear(512, 512),   # hidden layer\n",
        "            nn.ReLU(),             # activation function\n",
        "            nn.Linear(512, 10)     # output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ig4eLvDuvp2",
        "outputId": "16aff1aa-a99a-4947-bde9-e2aae4a08df9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# A loss function measures how far the model’s predictions are from the correct labels (truth). It guides the training process.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "# An optimizer updates the model’s weights based on the computed gradients during backpropagation."
      ],
      "metadata": {
        "id": "lAiiTb5Ey1uG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # This gives you the total number of samples in the dataset (not just in one batch).\n",
        "\n",
        "    model.train()\n",
        "    # This sets the model in training mode.\n",
        "      # It activates dropout, batch norm, etc. if used.\n",
        "      # Not needed in inference mode (model.eval() is used for testing/validation).\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # batch: Index of the batch (0, 1, 2, ...)\n",
        "        # (X, y): A batch of inputs (X) and targets (y) from the dataloader\n",
        "\n",
        "        # Example shapes:\n",
        "        #   X: [64, 1, 28, 28]\n",
        "        #   y: [64]\n",
        "\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # This ensures the data is on the same device as the model (CPU or GPU).\n",
        "        # Without this, it may causes an error during training\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        # Feeds the batch of inputs X through the model.\n",
        "        # Returns logits (raw output scores) of shape [batch_size, 10].\n",
        "\n",
        "        loss = loss_fn(pred, y)\n",
        "        # Compares predictions (pred) with actual labels (y)\n",
        "        # Returns the average loss for this batch (a single number).\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Calculates the gradient of the loss w.r.t. each model parameter (using the chain rule).\n",
        "        # These gradients are stored in parameter.grad.\n",
        "\n",
        "        optimizer.step()\n",
        "        # Uses the gradients calculated in loss.backward() to update the model's parameters.\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Clears the previous gradients, which accumulate by default in PyTorch.\n",
        "        # Must do this before the next batch.\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "-Ak-xwm7BX7i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # size: Total number of samples in the test dataset (e.g., 10,000 for FashionMNIST)\n",
        "    # num_batches: Total number of batches in the test set (e.g., 10,000 / 64 = 157)\n",
        "\n",
        "    model.eval()\n",
        "    # This puts the model in evaluation mode:\n",
        "    #   Disables features like dropout or batch norm updating.\n",
        "    #   Ensures consistent and correct behavior during testing.\n",
        "\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "      # This tells PyTorch not to compute gradients, which:\n",
        "      #   Saves memory\n",
        "      #   Speeds up computations\n",
        "      # Gradients are only needed for training, not testing.\n",
        "\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            # Moves data to the correct device (CPU/GPU)\n",
        "            # Performs a forward pass with model(X) to get predictions\n",
        "\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # Computes the loss for this batch\n",
        "\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            # pred.argmax(1)\tFinds the index (class) with the highest score for each image\n",
        "            # == y\tCompares predicted labels to true labels\n",
        "            # .type(torch.float)\tConverts the Boolean results (True/False) to floats (1.0 or 0.0)\n",
        "            # .sum().item()\tSums how many were correct in this batch and converts it to float\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    # test_loss: Average loss across all test batches\n",
        "    # correct: Fraction of correct predictions (i.e., accuracy) Accuracy = Number of correct predictions / Total number of predictions\n",
        "\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "lgllIT3r1Ds4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxzm-Hsi0LDT",
        "outputId": "3fc6135c-3088-4ab4-92bc-80527172751b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.292456  [   64/60000]\n",
            "loss: 2.284010  [ 6464/60000]\n",
            "loss: 2.265569  [12864/60000]\n",
            "loss: 2.268365  [19264/60000]\n",
            "loss: 2.243257  [25664/60000]\n",
            "loss: 2.209792  [32064/60000]\n",
            "loss: 2.225791  [38464/60000]\n",
            "loss: 2.183168  [44864/60000]\n",
            "loss: 2.181906  [51264/60000]\n",
            "loss: 2.154438  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 38.5%, Avg loss: 2.147512 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.153708  [   64/60000]\n",
            "loss: 2.145299  [ 6464/60000]\n",
            "loss: 2.083499  [12864/60000]\n",
            "loss: 2.111970  [19264/60000]\n",
            "loss: 2.041953  [25664/60000]\n",
            "loss: 1.982592  [32064/60000]\n",
            "loss: 2.013886  [38464/60000]\n",
            "loss: 1.923629  [44864/60000]\n",
            "loss: 1.937470  [51264/60000]\n",
            "loss: 1.868976  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 57.5%, Avg loss: 1.862598 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.890956  [   64/60000]\n",
            "loss: 1.861298  [ 6464/60000]\n",
            "loss: 1.735366  [12864/60000]\n",
            "loss: 1.796638  [19264/60000]\n",
            "loss: 1.664651  [25664/60000]\n",
            "loss: 1.621276  [32064/60000]\n",
            "loss: 1.645065  [38464/60000]\n",
            "loss: 1.535346  [44864/60000]\n",
            "loss: 1.571671  [51264/60000]\n",
            "loss: 1.473697  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.6%, Avg loss: 1.487561 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.552434  [   64/60000]\n",
            "loss: 1.521028  [ 6464/60000]\n",
            "loss: 1.365952  [12864/60000]\n",
            "loss: 1.451906  [19264/60000]\n",
            "loss: 1.318505  [25664/60000]\n",
            "loss: 1.323502  [32064/60000]\n",
            "loss: 1.334553  [38464/60000]\n",
            "loss: 1.250670  [44864/60000]\n",
            "loss: 1.300387  [51264/60000]\n",
            "loss: 1.205725  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 1.228568 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.306977  [   64/60000]\n",
            "loss: 1.290569  [ 6464/60000]\n",
            "loss: 1.122199  [12864/60000]\n",
            "loss: 1.236451  [19264/60000]\n",
            "loss: 1.102713  [25664/60000]\n",
            "loss: 1.134042  [32064/60000]\n",
            "loss: 1.154264  [38464/60000]\n",
            "loss: 1.080345  [44864/60000]\n",
            "loss: 1.136948  [51264/60000]\n",
            "loss: 1.056499  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 1.073638 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "# test_data[0]: Gets the first image and its label from the dataset.\n",
        "# [0]: The image (x) – a 1 × 28 × 28 tensor.\n",
        "# [1]: The label (y) – an integer like 9.\n",
        "\n",
        "# x: The image\n",
        "# y: The actual label (e.g., 9 = \"Ankle boot\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ZRhYX30UNX",
        "outputId": "7d6974fe-68c0-4862-cb1c-6f8a6276652d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensors**\n",
        "\n",
        "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters."
      ],
      "metadata": {
        "id": "hxnT5Q2UC2ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors can be initialized in various ways. Take a look at the following examples:"
      ],
      "metadata": {
        "id": "iyhhxq03GGxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Directly from data\n",
        "# Tensors can be created directly from data. The data type is automatically inferred.\n",
        "\n",
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "\n",
        "print(x_data)\n",
        "print(x_data.shape)\n",
        "print(x_data.ndim)\n",
        "print(x_data.dtype)\n",
        "print(x_data.itemsize)\n",
        "print(x_data.device)\n",
        "# shape: Returns a tuple representing the shape (dimensions) of the array.\n",
        "# ndim: Returns the number of dimensions (axes) of the array.\n",
        "# dtype: Provides the data type of the array elements.\n",
        "# itemsize: Returns the size in bytes of each element\n",
        "# device: Device tensor is stored on.\n",
        "\n",
        "# By default, tensors are created on the CPU. We need to explicitly move tensors to the accelerator using .to method (after checking for accelerator availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!\n",
        "# We move our tensor to the current accelerator if available\n",
        "# if torch.accelerator.is_available():\n",
        "#     tensor = tensor.to(torch.accelerator.current_accelerator())\n",
        "print(\"------------------------------------------\")\n",
        "# From a NumPy array\n",
        "# Tensors can be created from NumPy arrays.\n",
        "\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print(x_np)\n",
        "\n",
        "print(\"------------------------------------------\")\n",
        "# From another tensor:\n",
        "# The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n",
        "\n",
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "print(\"------------------------------------------\")\n",
        "# With random or constant values:\n",
        "# shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
        "\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vED-g_K409o",
        "outputId": "b4cdd81e-80cf-4cca-c8a8-a3160bccbbea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "torch.Size([2, 2])\n",
            "2\n",
            "torch.int64\n",
            "8\n",
            "cpu\n",
            "------------------------------------------\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "------------------------------------------\n",
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.6909, 0.3353],\n",
            "        [0.6910, 0.3305]]) \n",
            "\n",
            "------------------------------------------\n",
            "Random Tensor: \n",
            " tensor([[0.3844, 0.4920, 0.2729],\n",
            "        [0.8826, 0.5658, 0.7343]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operations on Tensors**\n",
        "\n",
        "Different operations on tensors: https://docs.pytorch.org/docs/stable/torch.html"
      ],
      "metadata": {
        "id": "jg3tQLpwJKgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard numpy-like indexing and slicing:\n",
        "\n",
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "# tensor[..., -1] is equivalent to tensor[:, -1]\n",
        "tensor[:,1] = 0\n",
        "print(tensor)\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "# Joining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension.\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f-gfNVtC09t",
        "outputId": "41d955d1-5346-45b5-81cb-d9f733d56c30"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "------------------------------------------\n",
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arithmetic operations**"
      ],
      "metadata": {
        "id": "fmxmYu79Ps4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "# This performs matrix multiplication:\n",
        "  # @ is the operator version\n",
        "  # matmul() is the function version\n",
        "  # tensor.T returns the transpose of the tensor (swaps rows & columns)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7TMV6kXLBCp",
        "outputId": "eb064322-9d87-42c8-e421-e9fb3faaa550"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single-element tensors**\n",
        "\n",
        "If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():"
      ],
      "metadata": {
        "id": "vHjHKvQDSEzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg = tensor.sum()\n",
        "# This returns a scalar tensor (0-dimensional), like tensor(12.)\n",
        "\n",
        "agg_item = agg.item()\n",
        "# .item() extracts the numerical value from a 1-element tensor.\n",
        "# Converts tensor(12.) → 12.0 (Python float)\n",
        "\n",
        "print(agg_item, type(agg_item))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Q_panHRs62",
        "outputId": "964c1d27-7f77-4a10-92a1-d757d7cdf847"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0 <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In-place operations**\n",
        "\n",
        "Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x.\n",
        "\n",
        "In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged."
      ],
      "metadata": {
        "id": "ixZeKXiuTAK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0MXFocjSLdY",
        "outputId": "87f135dd-bb17-4afb-a795-22524e6230c5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bridge with NumPy**\n",
        "\n",
        "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other."
      ],
      "metadata": {
        "id": "8HVmwZ2aTXGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor to NumPy array\n",
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "# A change in the tensor reflects in the NumPy array.\n",
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "print(\"------------------------------------------\")\n",
        "# NumPy array to Tensor\n",
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "# Changes in the NumPy array reflects in the tensor.\n",
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAeQGGmBTEXu",
        "outputId": "81cf40f5-2fa2-41e6-df46-ecc9c890416b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n",
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n",
            "------------------------------------------\n",
            "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "n: [1. 1. 1. 1. 1.]\n",
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHvzXXMVTjqU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}