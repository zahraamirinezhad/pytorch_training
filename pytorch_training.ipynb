{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "16QKE5QIfO61"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quickstart"
      ],
      "metadata": {
        "id": "eFMJY2i0CsHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# root=\"data\"\t=> The directory where the dataset will be stored. If \"data\" doesn’t exist, it will be created.\n",
        "# train=True =>\tThis tells PyTorch to load the training set (60,000 images).\n",
        "# download=True =>\tIf the dataset is not already present, download it from the internet.\n",
        "# transform=ToTensor() =>\tApply a transformation that converts each image (PIL format) into a PyTorch tensor and normalizes pixel values to [0.0, 1.0]."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgPV4pdpj72h",
        "outputId": "c46b2615-a554-437b-d4f9-17d224d81d78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.7MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 203kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.75MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 23.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "# This sets the number of samples per batch.\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "# This wraps the training_data (which is a FashionMNIST dataset) in a DataLoader.\n",
        "# What DataLoader does:\n",
        "  # Batches the data into size batch_size\n",
        "  # Allows iterating over the dataset using a for loop\n",
        "  # Optionally shuffles or loads data in parallel (more on this later)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "# output [64, 1, 28, 28]: Each batch contains 64 grayscale images (1 channel, 28x28 pixels) and their 64 labels."
      ],
      "metadata": {
        "id": "HcuQto-uoeiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955e8962-5f0a-41e8-ceb5-80971fc444b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "# Checks if any hardware accelerator (GPU or Apple Silicon, etc.) is available.\n",
        "# If yes, it uses it (e.g., \"cuda\" for NVIDIA GPU, \"mps\" for Apple GPU).\n",
        "# If not, it defaults to \"cpu\".\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "  # define a class that inherits from nn.Module, which is the base class for all PyTorch models.\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        # nn.Flatten() converts the 2D image (28×28) into a 1D vector (784 values).\n",
        "        # Shape change: [batch_size, 1, 28, 28] ⟶ [batch_size, 784]\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512), # input layer\n",
        "            nn.ReLU(),             # activation function\n",
        "            nn.Linear(512, 512),   # hidden layer\n",
        "            nn.ReLU(),             # activation function\n",
        "            nn.Linear(512, 10)     # output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ig4eLvDuvp2",
        "outputId": "9619215c-dc05-4b8b-e0d0-c4f20975d2ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# A loss function measures how far the model’s predictions are from the correct labels (truth). It guides the training process.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "# An optimizer updates the model’s weights based on the computed gradients during backpropagation."
      ],
      "metadata": {
        "id": "lAiiTb5Ey1uG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # This gives you the total number of samples in the dataset (not just in one batch).\n",
        "\n",
        "    model.train()\n",
        "    # This sets the model in training mode.\n",
        "      # It activates dropout, batch norm, etc. if used.\n",
        "      # Not needed in inference mode (model.eval() is used for testing/validation).\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # batch: Index of the batch (0, 1, 2, ...)\n",
        "        # (X, y): A batch of inputs (X) and targets (y) from the dataloader\n",
        "\n",
        "        # Example shapes:\n",
        "        #   X: [64, 1, 28, 28]\n",
        "        #   y: [64]\n",
        "\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # This ensures the data is on the same device as the model (CPU or GPU).\n",
        "        # Without this, it may causes an error during training\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        # Feeds the batch of inputs X through the model.\n",
        "        # Returns logits (raw output scores) of shape [batch_size, 10].\n",
        "\n",
        "        loss = loss_fn(pred, y)\n",
        "        # Compares predictions (pred) with actual labels (y)\n",
        "        # Returns the average loss for this batch (a single number).\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Calculates the gradient of the loss w.r.t. each model parameter (using the chain rule).\n",
        "        # These gradients are stored in parameter.grad.\n",
        "\n",
        "        optimizer.step()\n",
        "        # Uses the gradients calculated in loss.backward() to update the model's parameters.\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Clears the previous gradients, which accumulate by default in PyTorch.\n",
        "        # Must do this before the next batch.\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "-Ak-xwm7BX7i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # size: Total number of samples in the test dataset (e.g., 10,000 for FashionMNIST)\n",
        "    # num_batches: Total number of batches in the test set (e.g., 10,000 / 64 = 157)\n",
        "\n",
        "    model.eval()\n",
        "    # This puts the model in evaluation mode:\n",
        "    #   Disables features like dropout or batch norm updating.\n",
        "    #   Ensures consistent and correct behavior during testing.\n",
        "\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "      # This tells PyTorch not to compute gradients, which:\n",
        "      #   Saves memory\n",
        "      #   Speeds up computations\n",
        "      # Gradients are only needed for training, not testing.\n",
        "\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            # Moves data to the correct device (CPU/GPU)\n",
        "            # Performs a forward pass with model(X) to get predictions\n",
        "\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # Computes the loss for this batch\n",
        "\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            # pred.argmax(1)\tFinds the index (class) with the highest score for each image\n",
        "            # == y\tCompares predicted labels to true labels\n",
        "            # .type(torch.float)\tConverts the Boolean results (True/False) to floats (1.0 or 0.0)\n",
        "            # .sum().item()\tSums how many were correct in this batch and converts it to float\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    # test_loss: Average loss across all test batches\n",
        "    # correct: Fraction of correct predictions (i.e., accuracy) Accuracy = Number of correct predictions / Total number of predictions\n",
        "\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "lgllIT3r1Ds4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxzm-Hsi0LDT",
        "outputId": "df99c0b8-19d5-4f65-abcc-1e64c4010eb7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.312341  [   64/60000]\n",
            "loss: 2.298501  [ 6464/60000]\n",
            "loss: 2.277543  [12864/60000]\n",
            "loss: 2.267805  [19264/60000]\n",
            "loss: 2.243675  [25664/60000]\n",
            "loss: 2.225574  [32064/60000]\n",
            "loss: 2.218122  [38464/60000]\n",
            "loss: 2.192314  [44864/60000]\n",
            "loss: 2.200360  [51264/60000]\n",
            "loss: 2.147817  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.4%, Avg loss: 2.147647 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.168075  [   64/60000]\n",
            "loss: 2.156399  [ 6464/60000]\n",
            "loss: 2.092921  [12864/60000]\n",
            "loss: 2.105268  [19264/60000]\n",
            "loss: 2.045977  [25664/60000]\n",
            "loss: 1.999271  [32064/60000]\n",
            "loss: 2.018935  [38464/60000]\n",
            "loss: 1.944988  [44864/60000]\n",
            "loss: 1.961071  [51264/60000]\n",
            "loss: 1.870013  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.7%, Avg loss: 1.870504 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.917784  [   64/60000]\n",
            "loss: 1.877927  [ 6464/60000]\n",
            "loss: 1.755903  [12864/60000]\n",
            "loss: 1.795235  [19264/60000]\n",
            "loss: 1.683306  [25664/60000]\n",
            "loss: 1.648238  [32064/60000]\n",
            "loss: 1.667374  [38464/60000]\n",
            "loss: 1.573613  [44864/60000]\n",
            "loss: 1.603057  [51264/60000]\n",
            "loss: 1.489745  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.3%, Avg loss: 1.509537 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.584598  [   64/60000]\n",
            "loss: 1.544403  [ 6464/60000]\n",
            "loss: 1.393445  [12864/60000]\n",
            "loss: 1.463878  [19264/60000]\n",
            "loss: 1.343956  [25664/60000]\n",
            "loss: 1.351708  [32064/60000]\n",
            "loss: 1.363136  [38464/60000]\n",
            "loss: 1.295216  [44864/60000]\n",
            "loss: 1.328321  [51264/60000]\n",
            "loss: 1.224606  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 1.252193 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.331829  [   64/60000]\n",
            "loss: 1.313112  [ 6464/60000]\n",
            "loss: 1.146027  [12864/60000]\n",
            "loss: 1.251143  [19264/60000]\n",
            "loss: 1.120137  [25664/60000]\n",
            "loss: 1.157857  [32064/60000]\n",
            "loss: 1.175109  [38464/60000]\n",
            "loss: 1.121282  [44864/60000]\n",
            "loss: 1.157158  [51264/60000]\n",
            "loss: 1.069772  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 1.091959 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "# test_data[0]: Gets the first image and its label from the dataset.\n",
        "# [0]: The image (x) – a 1 × 28 × 28 tensor.\n",
        "# [1]: The label (y) – an integer like 9.\n",
        "\n",
        "# x: The image\n",
        "# y: The actual label (e.g., 9 = \"Ankle boot\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ZRhYX30UNX",
        "outputId": "6eda412e-6cec-4fb2-d082-4f366ec71c37"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tensors\n",
        "\n",
        "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters."
      ],
      "metadata": {
        "id": "hxnT5Q2UC2ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors can be initialized in various ways. Take a look at the following examples:"
      ],
      "metadata": {
        "id": "iyhhxq03GGxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Directly from data\n",
        "# Tensors can be created directly from data. The data type is automatically inferred.\n",
        "\n",
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "\n",
        "print(x_data)\n",
        "print(x_data.shape)\n",
        "print(x_data.ndim)\n",
        "print(x_data.dtype)\n",
        "print(x_data.itemsize)\n",
        "print(x_data.device)\n",
        "# shape: Returns a tuple representing the shape (dimensions) of the array.\n",
        "# ndim: Returns the number of dimensions (axes) of the array.\n",
        "# dtype: Provides the data type of the array elements.\n",
        "# itemsize: Returns the size in bytes of each element\n",
        "# device: Device tensor is stored on.\n",
        "\n",
        "# By default, tensors are created on the CPU. We need to explicitly move tensors to the accelerator using .to method (after checking for accelerator availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!\n",
        "# We move our tensor to the current accelerator if available\n",
        "# if torch.accelerator.is_available():\n",
        "#     tensor = tensor.to(torch.accelerator.current_accelerator())\n",
        "print(\"------------------------------------------\")\n",
        "# From a NumPy array\n",
        "# Tensors can be created from NumPy arrays.\n",
        "\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print(x_np)\n",
        "\n",
        "print(\"------------------------------------------\")\n",
        "# From another tensor:\n",
        "# The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n",
        "\n",
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "print(\"------------------------------------------\")\n",
        "# With random or constant values:\n",
        "# shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
        "\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vED-g_K409o",
        "outputId": "a608b85a-e6ca-4f2e-9e47-6ba944eec102"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "torch.Size([2, 2])\n",
            "2\n",
            "torch.int64\n",
            "8\n",
            "cpu\n",
            "------------------------------------------\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "------------------------------------------\n",
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.9608, 0.3891],\n",
            "        [0.9872, 0.5223]]) \n",
            "\n",
            "------------------------------------------\n",
            "Random Tensor: \n",
            " tensor([[0.1146, 0.5007, 0.4718],\n",
            "        [0.0398, 0.1173, 0.9475]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operations on Tensors**\n",
        "\n",
        "Different operations on tensors: https://docs.pytorch.org/docs/stable/torch.html"
      ],
      "metadata": {
        "id": "jg3tQLpwJKgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard numpy-like indexing and slicing:\n",
        "\n",
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "# tensor[..., -1] is equivalent to tensor[:, -1]\n",
        "tensor[:,1] = 0\n",
        "print(tensor)\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "# Joining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension.\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f-gfNVtC09t",
        "outputId": "d96e3fc7-57f8-4f65-c86d-124822df3eb7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "------------------------------------------\n",
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arithmetic operations**"
      ],
      "metadata": {
        "id": "fmxmYu79Ps4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "# This performs matrix multiplication:\n",
        "  # @ is the operator version\n",
        "  # matmul() is the function version\n",
        "  # tensor.T returns the transpose of the tensor (swaps rows & columns)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7TMV6kXLBCp",
        "outputId": "144cbef3-98a4-40c7-d494-da4c9e5df124"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single-element tensors**\n",
        "\n",
        "If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():"
      ],
      "metadata": {
        "id": "vHjHKvQDSEzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg = tensor.sum()\n",
        "# This returns a scalar tensor (0-dimensional), like tensor(12.)\n",
        "\n",
        "agg_item = agg.item()\n",
        "# .item() extracts the numerical value from a 1-element tensor.\n",
        "# Converts tensor(12.) → 12.0 (Python float)\n",
        "\n",
        "print(agg_item, type(agg_item))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Q_panHRs62",
        "outputId": "e1691063-2f41-435a-d825-883cfad47452"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0 <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In-place operations**\n",
        "\n",
        "Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x.\n",
        "\n",
        "In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged."
      ],
      "metadata": {
        "id": "ixZeKXiuTAK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0MXFocjSLdY",
        "outputId": "d580f969-f98d-4a7d-a454-e80af96e590b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bridge with NumPy**\n",
        "\n",
        "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other."
      ],
      "metadata": {
        "id": "8HVmwZ2aTXGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor to NumPy array\n",
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "# A change in the tensor reflects in the NumPy array.\n",
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "print(\"------------------------------------------\")\n",
        "# NumPy array to Tensor\n",
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "# Changes in the NumPy array reflects in the tensor.\n",
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAeQGGmBTEXu",
        "outputId": "61259b3f-199a-40b8-fe85-94c38bcf5ad9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n",
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n",
            "------------------------------------------\n",
            "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "n: [1. 1. 1. 1. 1.]\n",
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets & DataLoaders\n",
        "\n",
        "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data.\n",
        "\n",
        "Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples."
      ],
      "metadata": {
        "id": "7wrbS5mzhLXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a Dataset\n",
        "# Here is an example of how to load the Fashion-MNIST dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "# We load the FashionMNIST Dataset with the following parameters:\n",
        "  # root is the path where the train/test data is stored,\n",
        "  # train specifies training or test dataset,\n",
        "  # download=True downloads the data from the internet if it’s not available at root.\n",
        "  # transform and target_transform specify the feature and label transformations"
      ],
      "metadata": {
        "id": "QHvzXXMVTjqU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Iterating and Visualizing the Dataset**\n",
        "\n",
        "We can index Datasets manually like a list: training_data[index]. We use matplotlib to visualize some samples in our training data."
      ],
      "metadata": {
        "id": "XC62wi7ps9-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "# Creates a new figure canvas (like a blank page).\n",
        "# figsize=(8, 8) sets the figure size in inches.\n",
        "\n",
        "cols, rows = 3, 3\n",
        "# You want to show a 3×3 grid of images = 9 total images\n",
        "\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    # Picks a random index and extracts it as a plain Python int\n",
        "\n",
        "    img, label = training_data[sample_idx]\n",
        "    # img shape: [1, 28, 28]\n",
        "    # label: integer from 0 to 9\n",
        "\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    # Adds a small subplot at position i in a 3×3 grid.\n",
        "\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "wRwDfzxPjYKj",
        "outputId": "b540a4a4-c969-4ad2-fb57-64a7be31fa88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing your data for training with DataLoaders**\n",
        "\n",
        "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
        "DataLoader is an iterable that abstracts this complexity for us in an easy API."
      ],
      "metadata": {
        "id": "x2nDZusqtT5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "# We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled.\n",
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "cqRLn19ljiru",
        "outputId": "de8b72ca-d5b2-48be-9864-8f3352daca53"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transforms\n",
        "Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n",
        "\n",
        "All TorchVision datasets have two parameters -transform to modify the features and target_transform to modify the labels - that accept callables containing the transformation logic."
      ],
      "metadata": {
        "id": "LqzL4Q0MtfrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")\n",
        "\n",
        "# It is loading the FashionMNIST training dataset, and:\n",
        "#   Applying a transformation to images → convert to tensors\n",
        "#   Applying a transformation to labels → convert to one-hot encoded vectors\n",
        "\n",
        "# transform=ToTensor():\n",
        "#   Converts the PIL image (from dataset) into a PyTorch tensor of shape [1, 28, 28] (grayscale image)\n",
        "#   Scales pixel values from [0, 255] to [0.0, 1.0]\n",
        "\n",
        "# target_transform=Lambda(...):\n",
        "#   It is transforming integer labels (e.g., 5) into one-hot encoded vectors like: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
        "\n",
        "print(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu_i25JyqRzU",
        "outputId": "250dc11d-86b9-46bd-b7f4-5ea382e575fa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Target transform: Lambda()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the Neural Network"
      ],
      "metadata": {
        "id": "gLp0J-R84Er9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Get Device for Training\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "# We want to be able to train our model on an accelerator such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n",
        "\n",
        "# Define the Class\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        # logits are the raw output scores for each of the 10 classes.\n",
        "        return logits\n",
        "# We define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n",
        "\n",
        "# We create an instance of NeuralNetwork, and move it to the device, and print its structure.\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "# nn.ReLU(): Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
        "\n",
        "# nn.Linear(): The linear layer is a module that applies a linear transformation on the input using its stored weights and biases.\n",
        "\n",
        "# nn.Sequential(): nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined.\n",
        "\n",
        "# nn.Flatten(): We initialize the nn.Flatten layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsZD8BKq0-fQ",
        "outputId": "2683b0f7-7fc7-4b7c-c902-8deff72fa286"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the model, we pass it the input data. This executes the model’s forward, along with some background operations.\n",
        "# Do not call model.forward() directly!\n",
        "# Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the nn.Softmax module.\n",
        "X = torch.rand(1, 28, 28, device=device)\n",
        "# Creates a random dummy image with shape [1, 28, 28]\n",
        "# Batch size = 1\n",
        "# Random values in the range [0, 1]\n",
        "\n",
        "logits = model(X)\n",
        "print(logits)\n",
        "# Passes the dummy image through the model\n",
        "# Output: raw scores (logits), shape [1, 10]\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "print(pred_probab)\n",
        "# Converts logits into probabilities (between 0 and 1) using softmax\n",
        "# dim=1 → applies softmax across class scores\n",
        "\n",
        "y_pred = pred_probab.argmax(1)\n",
        "# argmax(1) selects the class with the highest probability\n",
        "\n",
        "print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "# nn.Softmax(dim=1):\n",
        "# The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYDP1oHcLaPG",
        "outputId": "538f8fda-f9c8-4231-edbe-90160748bae3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0027,  0.0127, -0.0697, -0.0195, -0.0921, -0.0414,  0.0081,  0.0156,\n",
            "          0.0345, -0.0387]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([[0.1016, 0.1032, 0.0950, 0.0999, 0.0929, 0.0977, 0.1027, 0.1035, 0.1055,\n",
            "         0.0980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Predicted class: tensor([8], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Parameters**\n",
        "\n",
        "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\n",
        "\n",
        "In this example, we iterate over each parameter, and print its size and a preview of its values."
      ],
      "metadata": {
        "id": "X6hJuXHlR9gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCDm-jzLMyT1",
        "outputId": "1a2b85ee-af8c-4405-d33c-a68e8b7553c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0032,  0.0077,  0.0180,  ...,  0.0109,  0.0246,  0.0221],\n",
            "        [ 0.0107,  0.0129, -0.0016,  ..., -0.0139,  0.0158,  0.0136]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0292,  0.0250], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0158, -0.0077, -0.0345,  ...,  0.0321, -0.0181,  0.0031],\n",
            "        [ 0.0108, -0.0298, -0.0267,  ..., -0.0223,  0.0133, -0.0392]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0426, -0.0236], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0005, -0.0234, -0.0142,  ...,  0.0398, -0.0130, -0.0113],\n",
            "        [-0.0218,  0.0281, -0.0116,  ...,  0.0339, -0.0051, -0.0164]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0127, -0.0354], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters\n",
        "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates.\n",
        "\n",
        "We define the following hyperparameters for training:\n",
        "\n",
        "*   Number of Epochs - the number of times to iterate over the dataset\n",
        "\n",
        "*   Batch Size - the number of data samples propagated through the network before the parameters are updated\n",
        "\n",
        "*   Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AS4Wt05pjBJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimization Loop\n",
        "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\n",
        "\n",
        "Each epoch consists of two main parts:\n",
        "*   The Train Loop - iterate over the training dataset and try to converge to optimal parameters.\n",
        "\n",
        "*   The Validation/Test Loop - iterate over the test dataset to check if model performance is improving."
      ],
      "metadata": {
        "id": "j4xNTiT6kqHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Function\n",
        "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
        "\n",
        "Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss."
      ],
      "metadata": {
        "id": "sfKGWydzlLgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer\n",
        "Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "Inside the training loop, optimization happens in three steps:\n",
        "*   Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "\n",
        "*   Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "\n",
        "*   Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
      ],
      "metadata": {
        "id": "Is7i-ybZlets"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Implementation\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "rLRhwvS-SE6G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "bwI-T2AwnfCk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn84YvP6mEeQ",
        "outputId": "624055ad-97ae-4092-a0a8-0f9073125fbf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.295093  [   64/60000]\n",
            "loss: 2.290995  [ 6464/60000]\n",
            "loss: 2.270504  [12864/60000]\n",
            "loss: 2.265166  [19264/60000]\n",
            "loss: 2.252657  [25664/60000]\n",
            "loss: 2.223448  [32064/60000]\n",
            "loss: 2.224851  [38464/60000]\n",
            "loss: 2.195570  [44864/60000]\n",
            "loss: 2.189180  [51264/60000]\n",
            "loss: 2.169596  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.4%, Avg loss: 2.157479 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.156834  [   64/60000]\n",
            "loss: 2.160539  [ 6464/60000]\n",
            "loss: 2.101710  [12864/60000]\n",
            "loss: 2.122900  [19264/60000]\n",
            "loss: 2.077777  [25664/60000]\n",
            "loss: 2.014958  [32064/60000]\n",
            "loss: 2.043661  [38464/60000]\n",
            "loss: 1.965945  [44864/60000]\n",
            "loss: 1.967569  [51264/60000]\n",
            "loss: 1.916903  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.0%, Avg loss: 1.903975 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.922437  [   64/60000]\n",
            "loss: 1.911190  [ 6464/60000]\n",
            "loss: 1.788479  [12864/60000]\n",
            "loss: 1.836766  [19264/60000]\n",
            "loss: 1.736729  [25664/60000]\n",
            "loss: 1.673482  [32064/60000]\n",
            "loss: 1.703846  [38464/60000]\n",
            "loss: 1.594940  [44864/60000]\n",
            "loss: 1.619997  [51264/60000]\n",
            "loss: 1.534633  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.9%, Avg loss: 1.537095 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.590402  [   64/60000]\n",
            "loss: 1.571215  [ 6464/60000]\n",
            "loss: 1.410580  [12864/60000]\n",
            "loss: 1.490591  [19264/60000]\n",
            "loss: 1.378756  [25664/60000]\n",
            "loss: 1.358435  [32064/60000]\n",
            "loss: 1.388192  [38464/60000]\n",
            "loss: 1.295186  [44864/60000]\n",
            "loss: 1.333463  [51264/60000]\n",
            "loss: 1.253047  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 1.264122 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.331449  [   64/60000]\n",
            "loss: 1.326947  [ 6464/60000]\n",
            "loss: 1.152669  [12864/60000]\n",
            "loss: 1.261509  [19264/60000]\n",
            "loss: 1.145302  [25664/60000]\n",
            "loss: 1.157226  [32064/60000]\n",
            "loss: 1.196159  [38464/60000]\n",
            "loss: 1.112566  [44864/60000]\n",
            "loss: 1.153074  [51264/60000]\n",
            "loss: 1.090152  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.095981 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.159422  [   64/60000]\n",
            "loss: 1.173850  [ 6464/60000]\n",
            "loss: 0.984508  [12864/60000]\n",
            "loss: 1.117857  [19264/60000]\n",
            "loss: 1.002227  [25664/60000]\n",
            "loss: 1.022498  [32064/60000]\n",
            "loss: 1.075619  [38464/60000]\n",
            "loss: 0.996077  [44864/60000]\n",
            "loss: 1.035914  [51264/60000]\n",
            "loss: 0.985889  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.0%, Avg loss: 0.986929 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.040117  [   64/60000]\n",
            "loss: 1.073753  [ 6464/60000]\n",
            "loss: 0.869216  [12864/60000]\n",
            "loss: 1.021387  [19264/60000]\n",
            "loss: 0.912255  [25664/60000]\n",
            "loss: 0.926676  [32064/60000]\n",
            "loss: 0.995078  [38464/60000]\n",
            "loss: 0.919554  [44864/60000]\n",
            "loss: 0.955496  [51264/60000]\n",
            "loss: 0.914869  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.2%, Avg loss: 0.912503 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.952700  [   64/60000]\n",
            "loss: 1.003907  [ 6464/60000]\n",
            "loss: 0.786625  [12864/60000]\n",
            "loss: 0.953519  [19264/60000]\n",
            "loss: 0.852251  [25664/60000]\n",
            "loss: 0.856234  [32064/60000]\n",
            "loss: 0.937543  [38464/60000]\n",
            "loss: 0.868368  [44864/60000]\n",
            "loss: 0.898066  [51264/60000]\n",
            "loss: 0.863610  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.859206 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.885961  [   64/60000]\n",
            "loss: 0.951717  [ 6464/60000]\n",
            "loss: 0.725313  [12864/60000]\n",
            "loss: 0.903765  [19264/60000]\n",
            "loss: 0.809595  [25664/60000]\n",
            "loss: 0.803657  [32064/60000]\n",
            "loss: 0.894198  [38464/60000]\n",
            "loss: 0.833053  [44864/60000]\n",
            "loss: 0.855819  [51264/60000]\n",
            "loss: 0.824624  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.6%, Avg loss: 0.819194 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.833285  [   64/60000]\n",
            "loss: 0.909993  [ 6464/60000]\n",
            "loss: 0.678137  [12864/60000]\n",
            "loss: 0.865792  [19264/60000]\n",
            "loss: 0.777273  [25664/60000]\n",
            "loss: 0.763433  [32064/60000]\n",
            "loss: 0.859503  [38464/60000]\n",
            "loss: 0.807431  [44864/60000]\n",
            "loss: 0.823007  [51264/60000]\n",
            "loss: 0.793719  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 0.787673 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJFgqP6jmGfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}